---
title: Spaceship Titanic Investigation
author: 
  - name: Audrey Saidel
    email: audrey.saidel@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 3/8/2025
date-modified: today
date-format: long
theme: flatly
toc: true
code-fold: true
---

```{r setup}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(ggridges)
library(marginaleffects)

data <- read_csv("https://raw.githubusercontent.com/agmath/agmath.github.io/master/data/classification/spaceship_titanic.csv")

names(data) <- janitor::make_clean_names(names(data))

data <- data %>%
  mutate(
    transported = factor(transported)
  )

data_splits <- initial_split(data, prop = 0.8, strata = transported)

train <- training(data_splits)
test <- testing(data_splits)

unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
```

## Statement of Purpose

Like its namesake vessel, the *Spaceship Titanic* encountered tragedy when several of its passengers were warped to an alternate dimension during flight! This analysis is a post-mortem on the flight and passenger list so that we may better understand who is at risk for interdimensional transport during spaceflight and can take future precautionary measures.

## Executive Summary

This document's goal is to understand whether logistic regression or vector support machines would be better for accuracy on the recall metric. This will be done by first an exploratory data analysis, model construction (machine and statistical, with hyperperameter tuning, and then a conclusion with a result analysis.

## Introduction

The year is 2063. We’ve come a long way from the early 2020’s, where billionaire tech entrepreneurs launched tiny rockets, holding a handful of celebrities or wealthy elites, into near-Earth orbit for an exorbitant pricetag. The future is now…well, was last week… Things are much more uncertain now. We were so excited with the launch of the Spaceship Titanic. It was supposed to be the beginning of a new era – affordable, long-range space travel for everyone. In hindsight, perhaps naming the thing the Titanic was a poor decision – too tempting for fate and karma.

In any case, space travel is an important venture for humanity at this point in our history as a species. Demand is high, even with last week’s disaster. We have a vested interest in safe and reliable travel through the cosmos and need to better understand what happened to the travelers who’ve disappeared and why it happened to them and not other passengers. Demand for space travel was expected to reach 86 million travelers next year – we can’t continue if we only expect 43 million passengers to arrive at their intended destination.

## Exploratory Data Analysis

The original data set on the passengers contained `r data %>% nrow()` passengers and `r data %>% ncol()` features (variables). We can see the first-few passengers'-worth of data printed out below.

```{r}
data %>%
  head() %>%
  kable()
```

That data was split into a collection of `r train %>% nrow()` training observations and `r test %>% nrow()` test observations for validating our final model's performance. Care was taken to ensure that the passengers who were *transported* to an alternate dimension are proportionally represented across the *training* and *test* sets.

### Univariate Analyses

Since our goal is to understand who was transported to an alternate dimension during flight and perhaps gain some insight as to why they were transported, we’ll start by understanding the transported variable and the distributions of the other variables available to us.

```{r}
train %>%
  ggplot() + 
  geom_bar(aes(x = transported)) + 
  labs(title = "Distribution of Transported Passengers",
       x = "Interdimensional Transport Status",
       y = "Count")
```

```{r}
pct_transported <- train %>%
  count(transported) %>%
  ungroup() %>%
  mutate(pct = 100*n/sum(n)) %>%
  filter(transported == "yes") %>%
  pull(pct)
```

The percentage of passengers transported in the training set is about `r round(pct_transported, 2)`%. Let’s look at the distributions of the other categorical variables in the data set.

```{r}
p1 <- train %>%
  ggplot() + 
  geom_bar(aes(x = home_planet)) + 
  labs(
    title = "Boarding Planet",
    x = "",
    y = "Count"
  ) + 
  coord_flip()

p2 <- train %>%
  ggplot() + 
  geom_bar(aes(x = cryo_sleep)) +
  labs(
    title = "CryoSleep Selection",
    y = "Count",
    x = ""
  )

p3 <- train %>%
  ggplot() + 
  geom_bar(aes(x = destination)) + 
  labs(
    title = "Destination Planet",
    x = "",
    y = "Count"
  ) + 
  coord_flip()

p4 <- train %>%
  ggplot() + 
  geom_bar(aes(x = vip)) + 
  labs(
    title = "VIP Status",
    x = "",
    y = "Count"
  )

(p1 + p2) / (p3 + p4)
```

From the top-left plot, we see that the majority of passengers board on Earth, while fewer passengers board on Europa and Mars. Some passengers have no boarding planet information (`NA`) – perhaps these passengers are crew members. In the top-right plot, we see that nearly 2/3 of passengers choose the Cryo Sleep option, while around 1/3 do not. Again, some passengers have missing data here. The distribution of destination planet is shown in the lower-right, and tells us that the most popular destination (by a large margin) is TRAPPIST-1e. The only other two destination planets are PSO J318.5-22 and 55 Cancri e. As in the previous plots, some passengers do not have an identified destination. Finally, the proportion of passengers with VIP status is about 2.23.

In each of the plots, we identified several passengers with missing values. There are 0 passengers missing information for all four of these variables. This means that our earlier conjecture about those passengers being crew is unlikely.

Let’s continue on to view the distributions of the numerical predictors available to us. We’ll start with the distribution of passenger ages.

```{r}
#| message: false
#| warning: false

train %>%
  ggplot() + 
  geom_histogram(aes(x = age), color = "black",
                 fill = "purple") + 
  labs(
    title = "Passenger Ages",
    x = "Age (Years)",
    y = ""
  )
```

The plot above shows a [near] 0-inflated distribution. That is, there is an inflated number of observations near 0, given the shape of the rest of the distribution. Ages are right-skewed, with a median passenger age of 27 years old. Next we’ll look at the distribution of room service charges.

```{r}
#| message: false
#| warning: false

p1 <- train %>%
  ggplot() + 
  geom_density(aes(x = room_service),
               fill = "purple",
               color = "black") + 
  geom_boxplot(aes(x = room_service, y = -0.005),
               fill = "purple",
               width = 0.002) + 
  labs(
    title = "Room Service Money Spent",
    x = "Expenditure",
    y = ""
    )

p2 <- train %>%
  ggplot() + 
  geom_density(aes(x = room_service),
               fill = "purple",
               color = "black") + 
  geom_boxplot(aes(x = room_service, y = -0.05),
               fill = "purple",
               width = 0.02) + 
  labs(
    title = "Room Service Money Spent",
    x = "Expenditure",
    y = ""
    ) + 
  scale_x_log10()

p1 + p2
```

Both of the plots above show the distribution of room service expenditures. From the plot on the left, we can see that the distribution is very strongly right-skewed. The majority of passengers spent very little on room service, but there were some passengers who ran up extremely large tabs! The plot on the right shows the same variable but on a logarithmic scale. This particular transformation ignores passengers who did not spend any money on room service. From this plot, we actually see that the median room service expenditure among passengers who utilized room service is quite high – it is about `r median(train$room_service)`. We’ll continue our exploration of the available numerical features below, by working with the expenditures at the food court, shopping mall, spa, and VR deck. All of these are right skewed so we’ll just show the distributions on a logarithmic scale.

```{r}
#| warning: false
#| messae: false

p_food <- train %>%
  ggplot() + 
  geom_density(aes(x = food_court), fill = "purple") + 
  geom_boxplot(aes(x = food_court, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "Food Court Expenditures",
       x = "Money Spent",
       y = "")

p_shop <- train %>%
  ggplot() + 
  geom_density(aes(x = shopping_mall), fill = "purple") + 
  geom_boxplot(aes(x = shopping_mall, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "Shopping Mall Expenditures",
       x = "Money Spent",
       y = "")

p_spa <- train %>%
  ggplot() + 
  geom_density(aes(x = spa), fill = "purple") + 
  geom_boxplot(aes(x = spa, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "Spa Expenditures",
       x = "Money Spent",
       y = "")

p_vr <- train %>%
  ggplot() + 
  geom_density(aes(x = vr_deck), fill = "purple") + 
  geom_boxplot(aes(x = vr_deck, y = -0.075), 
               fill = "purple", width = 0.05) +
  scale_x_log10() +
  labs(title = "VR Deck Expenditures",
       x = "Money Spent",
       y = "")

(p_food + p_shop) / (p_spa + p_vr)
```

The distributions of these variables are all quite similar to one another. The distributions are skewed and 0-inflated. The distributions remain left-skewed even when plotted on a logarithmic scale and the observations at 0 are removed. The mean, median, standard deviation, and interquartile range for each expenditure venue are reported below without the removal of those zero observations.

```{r}
train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  select(Venue, Expenditure) %>%
  group_by(Venue) %>%
  summarize(mean_expenditure = mean(Expenditure, na.rm = TRUE),
            median_expenditure = median(Expenditure, na.rm = TRUE),
            sd_expenditure = sd(Expenditure, na.rm = TRUE),
            iqr_expenditure = IQR(Expenditure, na.rm = TRUE)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

The same metrics are reported below after removal of the zero expenditure values. That is, the summary metrics reported below consider only passengers who utilized the corresponding services. These values will align with measures indicated from the log-scale plots above.

```{r}
train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  select(Venue, Expenditure) %>%
  filter(Expenditure > 0) %>%
  group_by(Venue) %>%
  summarize(mean_expenditure = mean(Expenditure, na.rm = TRUE),
            median_expenditure = median(Expenditure, na.rm = TRUE),
            sd_expenditure = sd(Expenditure, na.rm = TRUE),
            iqr_expenditure = IQR(Expenditure, na.rm = TRUE)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

Below is a visual representation of the distributions for these expenditure variables with the observations at 0 expenditure removed.

```{r}
#| message: false
#| warning: false

train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  select(Venue, Expenditure) %>%
  filter(Expenditure > 0) %>%
  ggplot() + 
  geom_density_ridges(aes(x = Expenditure, y = Venue, fill = Venue),
                      scale = 0.5) + 
  geom_boxplot(aes(x = Expenditure, fill = Venue, y = Venue), width = 0.05) + 
  scale_x_log10(labels = scales::dollar_format()) + 
  labs(title = "Expenditure Distributions",
       x = "Money Spent",
       y = "") + 
  theme(legend.position = "None")
```

### Multivariate Analyses

Now that we understand the individual distributions of the variables, its time to look at how these predictors are associated with out response variable (transported). We’ll begin by looking for associations between transported and the categorical variables.

```{r}
p_home <- train %>%
  ggplot() + 
  geom_bar(aes(y = home_planet,
               fill = transported),
           position = "dodge",
           show.legend = FALSE) + 
  labs(title = "Home Planet and Transport",
       x = "Count",
       y = "")

p_cryo <- train %>%
  ggplot() + 
  geom_bar(aes(x = cryo_sleep,
               fill = transported),
           position = "dodge") + 
  labs(title = "Cryo Sleep and Transport",
       x = "",
       y = "Count")

p_destination <- train %>%
  ggplot() + 
  geom_bar(aes(y = destination,
               fill = transported),
           position = "dodge",
           show.legend = FALSE) + 
  labs(title = "Destination and Transport",
       x = "Count",
       y = "")

p_vip <- train %>%
  ggplot() + 
  geom_bar(aes(x = vip,
               fill = transported),
           position = "fill",
           show.legend = FALSE) + 
  labs(title = "VIP Status and Transport",
       x = "",
       y = "Proportion")

(p_home + p_cryo) / (p_destination + p_vip)
```

From the four plots above, we have the following takeaways. First, the plot on the left shows the passengers from Europa were much more likely to be transported than passengers from Mars or Earth. Passengers from Earth had a less than 50% transport rate while passengers from Mars had a slightly larger than 50% transport rate. Passengers in Cryo Sleep had an extremely elevated likelihood of transport than those who did not take advantage of Cryo Sleep. There were slight differences in transport rates by destination and by VIP status, but the choice to undergo Cryo Sleep seems to have been the largest influence over whether passengers were transported or not.

Now we’ll consider how the numerical features may be associated with the `transported` status of passengers.

```{r}
#| message: false
#| warning: false

train %>%
  pivot_longer(cols = c("room_service", "food_court", "shopping_mall", "spa", "vr_deck"), 
               names_to = "Venue",
               values_to = "Expenditure") %>%
  mutate(transported = ifelse(transported == "yes", "transported", "not")) %>%
  ggplot() + 
  geom_boxplot(aes(x = Expenditure,
                   y = transported,
                   fill = Venue),
               show.legend = FALSE) +
  scale_x_log10() +
  facet_wrap(~Venue) + 
  labs(title = "",
       x = "Expenditure",
       y = "")
```

In the group of plots appearing above, we see that higher food court and shopping mall expenditures were associate with those passengers who were transported than those who were not. Those individuals not being transported had higher room service, spa, and VR deck expenditures on average than those who were not transported.

As a result of this exploratory analysis, we’ve identified several important insights as we proceed to the model construction phase of this analysis. Firstly, about half of passengers were transported to an alternate dimension while the other half were transported safely. All of the numerical features are very heavily right-skewed aside from age. The variable most strongly associated with whether or not a passenger was transported may be the choice to Cryo Sleep during the flight. Other variables showed associations as well, but were less pronounced.

## Model Construction and Assessment


### Statistical Learning

We will now use statistical learning through the lens of logistic regression. We are trying to determine which individuals have the highest chance of being transported off the spaceship to another location. 

#### Logistic Regression

```{r}
# Build specification
log_reg_spec <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")

# Recipe
log_reg_rec <- recipe(transported ~ ., data = train) |>
  step_rm(passenger_id) |>
  step_rm(name) |>
  step_rm(cabin)|>  # Ignore it for now 
  step_impute_median(all_numeric_predictors()) |> # Mean is too sensitive to outliers to use
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors())

# Workflow
log_reg_wf <- workflow() |>
  add_model(log_reg_spec) |>
  add_recipe(log_reg_rec) 

# Fit it
log_reg_fit <- log_reg_wf |>
  fit(train)

log_reg_fit |>
extract_fit_engine() |>
tidy() |> # Data frame
mutate(
  odds_ratio = exp(estimate),
  oods_lower = exp(estimate - 2*std.error), # 95% Confidence interval around the estimate
  odds_upper = exp(estimate + 2*std.error)
)
```
```{r}
complex_log_reg_rec <- log_reg_rec %>%
  step_interact(~ food_court:starts_with("home_planet"))

complex_log_reg_wf <- workflow() %>%
  add_model(log_reg_spec) %>%
  add_recipe(complex_log_reg_rec)

complex_log_reg_fit <- complex_log_reg_wf %>%
  fit(train)

complex_log_reg_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  kable()
```

```{r}
new_data <- crossing(
  passenger_id = NA,
  home_planet = "Europa",
  cryo_sleep = "no",
  cabin = NA,
  destination = "TRAPPIST-1e",
  age = mean(train$age, na.rm = TRUE),
  vip = "no",
  room_service = mean(train$room_service, na.rm = TRUE),
  food_court = mean(train$food_court, na.rm = TRUE),
  shopping_mall = mean(train$shopping_mall, na.rm = TRUE),
  spa = mean(train$spa, na.rm = TRUE),
  vr_deck = seq(min(train$vr_deck, na.rm = TRUE),
            max(train$vr_deck, na.rm = TRUE),
            by = 1),
  name = NA
)

new_data_baked <- complex_log_reg_rec %>%
  prep() %>%
  bake(new_data)

mfx <- slopes(log_reg_fit %>% extract_fit_parsnip(),
              newdata = new_data_baked,
              variables = "vr_deck",
              type = "prob") %>%
  tibble()


mfx %>%
  filter(group == "yes") %>%
  select(vr_deck, estimate, conf.low, conf.high) %>%
  ggplot() +
  geom_line(aes(x = vr_deck, y = estimate), color = "purple", lty = "dashed", lwd = 1.5) +
  geom_ribbon(aes(x = vr_deck, ymin = conf.low, ymax = conf.high),
              fill = "grey", alpha = 0.5) +
  labs(x = "VR Deck",
       y = "Marginal Effect",
       title = "Marginal Effects of Unit Increase in VR Deck Spending") + 
  theme_bw()

mfx %>%
  filter(group == "yes") %>%
  select(vr_deck, estimate, conf.low, conf.high) %>%
  ggplot() +
  geom_line(aes(x = vr_deck, y = estimate), color = "purple", lty = "dashed", lwd = 1.5) +
  geom_ribbon(aes(x = vr_deck, ymin = conf.low, ymax = conf.high),
              fill = "grey", alpha = 0.5) +
  labs(x = "VR Deck",
       y = "Marginal Effect",
       title = "Marginal Effects of Unit Increase in VR Deck Spending") + 
  coord_cartesian(xlim = c(0, 5000)) + 
  theme_bw()
```

#### Confusion Matrix

```{r}
log_reg_fit |>
  augment(train) |> # Attaches predictions to a data set
  conf_mat(transported, .pred_class) # Compare the truth to the prediction
```
```{r}
# Created a new column called correct-flag, writes how the prediction was incorrect or yes if we were correct
log_reg_fit |> # Regression model that can help predict data
  augment(train) |> 
  mutate(
   # correct_flag = ifelse(transported == .pred_class, "yes", ifelse(transported == "yes", "false-negative", "false-positive"))
 # ) |>
  correct_flag = case_when(transported == .pred_class ~ "yes", transported == "yes" ~ "false-negative", TRUE ~ "false_positive")
  ) |>
  group_by(correct_flag) |>
  summarize(median_food_spend = median(food_court, na.rm = TRUE))

  
```


### Machine Learning

If a person or observation belongs to a class we worry about (the transported class) we want to build a model that identifies them as often as possible, because we don't want to lose people. We are going the ethics route of maybe losing money by not selling a ticket to someone who actually wouldn't get transported, but we are putting safety first.

```{r}
train_folds <- vfold_cv(train, v = 10, strata = transported)
```


We are going to pay attention to RECALL.

#### Logistic Regression

Mean is the approximated accuracy for each metric. Accuracy itself is around 79%, precision will be around 79%, and recall will be about 77%.

```{r}
# Set metrics
my_metrics <- metric_set(accuracy, precision, recall)

# Cross Validate this base model
log_reg_cv_results <- log_reg_wf |>
  fit_resamples(
    resamples = train_folds, metrics = my_metrics
  )

# Collect the results, the results are not a fitted model, only ten sets of performance metrics
log_reg_cv_results |>
  collect_metrics()
```

##### Hyperparameter Tuning
```{r}
log_reg_spec <- logistic_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet") |>
  set_mode("classification")

log_reg_tune_rec <-  recipe(transported ~ ., data = train) |>
  step_rm(passenger_id) |>
  step_rm(name) |>
  step_rm(cabin)|>  # Ignore it for now 
  step_impute_median(all_numeric_predictors()) |> # Mean is too sensitive to outliers to use
  step_impute_mode(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors())

log_reg_tune_wf <- workflow() |>
  add_model(log_reg_spec) |>
  add_recipe(log_reg_tune_rec)
```

Tuning our penalty and mixture hyper parameters
```{r}
# Let's tune our workflow over a random space filling grid
log_reg_tune_results <- log_reg_tune_wf |>
  tune_grid(
    resamples = train_folds,
    grid = 12,
    metrics = metric_set(accuracy, precision, recall)
  )

log_reg_tune_results |>
  show_best(n = 10, metric = "recall")

log_reg_seq_tune_results <- log_reg_tune_wf |>
  tune_bayes(
    resamples = train_folds,
    iter = 20, # How long we want to spend finding the best hyper parameters, process will stop after 10 w/ no improvement
    metrics = metric_set(recall),
    initial = log_reg_tune_results # Starts from the best hyper parameter from tuning result
    )

log_reg_seq_tune_results |>
  show_best(n = 10, metric = "recall")
```

### Vector Support Machine

Now, let's create a VSM to analyze the same metric as what we did in the logistic regression model. We can compare and contrast our model results after to see which one will be more accurate in predicting the recall metric.

```{r}
svm_linear_spec <- svm_linear(cost = tune()) |> # Linear SVM
  set_engine("LiblineaR") |>
  set_mode("classification")

svm_rec <- recipe(transported ~ ., data = train) |>
  step_rm(passenger_id) |>
  step_rm(name) |>
  step_rm(cabin)|> 
  step_rm(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors())

svm_wf <- workflow() |>
  add_model(svm_linear_spec) |>
  add_recipe(svm_rec)



```

##### Hyperparameter Tuning
```{r}
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

# Tuning
svm_bayes_results <- svm_wf |>
  tune_bayes(
    resamples = train_folds,
    iter = 20,  
    metrics = metric_set(recall),
    initial = 10
  )
tictoc::toc()

doParallel::stopImplicitCluster()
unregister()

svm_bayes_results |>
  collect_metrics() |>
  arrange(-mean)


```

```{r}
best_param_svm <- svm_bayes_results |>
  select_best()

best_svm <- svm_wf |>
  finalize_workflow(
  best_param_svm
)
best_svm_fit <- best_svm |>
  fit(train)
  
best_svm_fit |>
  extract_fit_parsnip() |>
  tidy()
``` 


## Conclusion

From looking at the mean on both the Support Vector Machine and the Logistic Regression Model, it is clear that the Logistic Regression Model is better at predicting transported victims. The SVM comes in at .7807, while the LRM is highest at .8702. There is a large percentage point difference, which means we should likely steer clear from using SVM for predictions. This is likely due to the lack of utilization of categorical variables in the SVM. Because the SVM measures distance, it isn't useful to use nominal predictors. However, in the LRM, the use of these categorical variables makes it more accurate when predicting the recall metric. 



