---
title: "CA1"
format: html
author: 
  - name: Audrey Saidel
    email: audrey.saidel@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
date: 1/11/2025
date-modified: 2/1/2025
title-block-banner: "#F9629F"
description: "Competition Assignment #1"
theme: journal
toc: true
execute:
  freeze: auto
code-fold: true
---

# Setup
```{r}
#| message: false
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
data <- read_csv("data.csv")
set.seed(555)
data_splits <- initial_split(data, 0.75, strata = priceRange)

train <- training(data_splits)
test <- testing(data_splits)

unregister()
```
# Statement of Purpose
Zillow is a real estate market website that lists houses for sale in both the US and Canada. Zillow carries data on sale prices, rent prices, mortgage prices, locations, and house sizing details. This project seeks to construct predictive models on the listed price range of a property in the US. This project will enhance transparency in the real estate market by displaying the factors that impact listing price range. This can help buyers make informed decisions when buying a house, as well as help sellers figure out a practical price point to list their house at.
# Exploratory Data Analysis
Before we begin the analysis, here is a snippet of the first six rows of the Zillow data we will be working with. In the data set, there are 7498 observations.
```{r}
data |>            # description is much too long
  mutate(description = str_trunc(description, width = 50)) |> 
  head() |>
  kable() |>
  kable_styling()
```


Strata will be used in the training data set due to an unbalanced lower proportion of houses in the 0-250000 price range.
```{r}
library(dplyr)
# Check distribution of price_range in the full dataset
data |>
  count(priceRange) |>
  mutate(proportion = n / sum(n)) |>
  kable() |>
  kable_styling()

```

## Price Range Distribution

This section maps the distribution of the priceRange variable. To begin with, pictured below is a chart displaying the summary of priceRange in our training data set this time.
```{r}
train |>
  count(priceRange) |>
  mutate(proportion = n / sum(n)) |>
  kable() |>
  kable_styling()
```

Below is a visual bar chart displaying the summary of priceRange in our training data set. From both the graph and the bar chart, it appears that there are more homes in our data set that have the price range of: 250000-350000 and fewer homes that have the price range of: 0-250000. This lines up with the proportions of the entire data set from earlier, and further proves why it was necessary to use strata. 
```{r}
ggplot(train, aes(x = priceRange, fill = priceRange)) +
  geom_bar() +
  theme_minimal() + 
  labs(title = "Number of Homes by Price Range", x = "Price Range", y = "Number of Homes") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Diagonal prices
```

The graph is reasonable, as the more average house prices (250000 to 650000) are the most populated with houses. The more extreme min and maximum price ranges have less houses, due to their rarity.

Now that we understand the distribution of price ranges better, it's time to look for the factors that may influence them the most.

## Price Range and City

Assuming the price range will vary by city is a reasonable guess, as some cities are known to be more expensive than others. To try and prove this, it is first best to look for how many properties exist in each city, so we can get a feel for the scope of the houses. We can do this buy creating a chart similar to the one we used for the count of priceRange.
```{r}
train %>%
  count(city) %>%
  mutate(proportion = n/sum(n)) %>%
  kable() %>%
  kable_styling()
```
```{r}
ggplot(train, aes(x = city, fill = city)) +
  geom_bar() +
  theme_minimal() + 
  labs(title = "Number of Homes by City", x = "City", y = "Number of Homes") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Diagonal city
```

Wow! It seems like 98% of our homes are located in Austin. Because of this horribly uneven and skewed distribution, it's not worth looking into if priceRange is effected by the city the house is in, since 98% of the houses are in the same city.

Let's try something different. 

## Price Range and Home Type
It is reasonable to suggest that the type of home may effect the price range the home is in due to size and accommodations. To prove this however, we will once again need a count of all the different home types.
```{r}
train |>
  count(homeType)|>
  mutate(proportion = n/sum(n)) |>
  kable() |>
  kable_styling()
```
```{r}
ggplot(train, aes(x = homeType, fill = homeType)) +
  geom_bar() +
  theme_minimal() + 
  labs(title = "Number of Homes by Home Type", x = "Home Type", y = "Number of Homes") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Diagonal homeType
```

Once again we have the same issue that happened when trying to correlate priceRange to city. The proportions are just too far off. 93% of houses in our training set are single family homes, so even if they were more expensive, it's unfair to justify it that way with such a skewed proportion.

Let's try something else.

## Price Range and Average School Rating
It would be interesting to see if there is a association between the average rating of a school, and the price range of a house. It is a generalized thought that the "nicer" (more expensive) the neighborhood is, the higher the rating for their schools. Let's see if this is true.
First, we should calculate the summary statistics for the average school rating, to make sure this is worth looking into. 
```{r}
train %>%
  summarize(
    min_rating = min(avgSchoolRating),
    median_rating = median(avgSchoolRating),
    avg_rating = mean(avgSchoolRating),
    max_rating = max(avgSchoolRating),
    sd_rating = sd(avgSchoolRating)
  ) %>%
  pivot_longer(everything(), 
               names_to = "Metric",
               values_to = "Value") %>%
  kable() %>%
  kable_styling()
```
```{r}
ggplot(train, aes(x = "", y = avgSchoolRating)) + 
  geom_boxplot(fill = "lightblue") +
  theme_minimal() + 
  labs(title = "Boxplot of Average School Rating", y = "Average School Rating")


```

Using the box plot and our summary statistic chart, we can see a clear spread of data that is similar to our proportions on price ranges from earlier. This is promising, let's find out if there is a correlation.

### Comparison

Using another box plot, we will compare average school rating to price range. As shown in the box plot below, there is a positive association between the two variables. It is more likely that the lower the rating for the school is, the lower the median price range is. There is not much change in the higher scale of the chart (450000+), but there is a clear climb in price that follows a climb in school rating. This variable will be able to help us in predicting priceRange. 
```{r}
ggplot(train, aes(x = priceRange, y = avgSchoolRating, fill = priceRange)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Comparison of Average School Rating by Price Range", x = "Price Range", y = "Average School Rating")

```
# Model Construction and Interpretation

We will now use K-Nearest Neighbors (KNN) and decision trees—both multi-class models—to predict house price ranges. To enhance prediction accuracy, we will apply cross-validation and hyper-parameter tuning.
  
  Let's begin with the decision tree.

## Decision Tree

  First we need to create the decision tree workflow.
```{r}
# Specification
dt_clf_spec <- decision_tree(min_n = tune(), tree_depth = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

# Recipe
dt_clf_rec <- recipe(priceRange ~., data = train) |>
  step_rm(description) |>
  step_rm(id) |>
  step_dummy(all_nominal(), -all_outcomes()) 
  

# Workflow
dt_clf_wf <- workflow() |>
  add_model(dt_clf_spec) |>
  add_recipe(dt_clf_rec)
```

### Tuning

  First we need to create cross-validation folds.Because we used strata previously, we'll use it again here. Once we have our folds, we can begin tuning.
```{r}
train_folds <- vfold_cv(train, v = 10, strata = priceRange)
```
```{r}

n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

dt_tune_results <- dt_clf_wf |>
  tune_bayes(
    resamples = train_folds,
    metrics = metric_set(mn_log_loss),
    initial = 10, 
    control = control_bayes(parallel_over = "everything")
  )

doParallel::stopImplicitCluster()
unregister()
```
```{r}
dt_tune_results |>
  show_best(n= 10)

dt_best_params <- dt_tune_results |>
  select_best(metric = "mn_log_loss")

dt_best_wf <- dt_clf_wf |>
  finalize_workflow(dt_best_params)

dt_best_fit <- dt_best_wf |>
  fit(train)
```

```{r}
log_loss <- augment(dt_best_fit, train) |>
  mutate(priceRange = as.factor(priceRange)) |>
  mn_log_loss(priceRange, starts_with(".pred"), - .pred_class)
```

```{r}
submission <- dt_best_fit |>
  augment(data) |>
  rename(
    prob_A = ".pred_0-250000",
    prob_B = ".pred_250000-350000",
    prob_C = ".pred_350000-450000",
    prob_D = ".pred_450000-650000",
    prob_E = ".pred_650000+",
  ) |>
  select(id, starts_with("pred"))

write.csv(submission, "submission.csv", row.names = FALSE)
```

## KNN
```{r}
# Specification
knn_clf_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Recipe
knn_clf_rec <- recipe(priceRange ~ ., data = train) |>
  step_normalize(all_numeric_predictors()) |>
  step_rm(all_nominal_predictors()) |>
  step_impute_median(all_numeric_predictors())
  
# Workflow
knn_wf <- workflow() %>%
  add_model(knn_clf_spec) %>%
  add_recipe(knn_clf_rec)


```

```{r}
n_cores <- parallel::detectCores()
cl <- parallel::makeCluster(n_cores - 1, type = "PSOCK")
doParallel::registerDoParallel(cl)

tictoc::tic()

knn_tune_results <- tune_bayes(
  knn_wf,
  resamples = train_folds,
  metrics = metric_set(mn_log_loss),
  initial = 10,  # Number of initial random grid points
  control = control_bayes(verbose = TRUE)
)

# View results
collect_metrics(knn_tune_results)


tictoc::toc()

doParallel::stopImplicitCluster()
unregister()
```

```{r}
best_knn_model <- knn_wf %>%
  finalize_workflow(select_best(knn_tune_results, metric = "mn_log_loss"))

# Fit the final model to the entire training data
final_knn_fit <- fit(best_knn_model, data = train)
```

