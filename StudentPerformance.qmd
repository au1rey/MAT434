---
title: Student Performance Investigation
author: 
  - name: Audrey Saidel
    email: audrey.saidel@snhu.edu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 3/31/2025
date-modified: 4/14/2025
title-block-banner: "#F9629F"
description: "Classification Final Project"
theme: journal
date-format: long
toc: true
code-fold: true
warning: false
---
## Setup
```{r setup}
#| message: false

library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
library(ggridges)
library(marginaleffects)
library(readr)
library(ggplot2)
library(yardstick)
library(xgboost)


# Load the data set
data <- read_csv("/Users/saide/Downloads/archive/Student_performance_data _.csv")

set.seed(555)
data_splits <- initial_split(data, prop = 0.8, strata = GradeClass)

train <- training(data_splits)
test <- testing(data_splits)



```

## Statement of Purpose

This is an analysis on a synthetic data set of student performance created by Rabie El Kharou on Kaggle. The purpose of the data set is to have an overview on how different factors influence student academic performance and overall grade. I will be using this data set in order to predict student grade based on factors such as parental education, support, extracurriculars, and more. 

## Executive Summary

The goal of this document is to predict a student's grade based on outside and inside factors. The document will begin with an exploratory analysis to see specifically which factors play a big role into overall grade and how these factors interact with each other. After this, model construction will begin using the already known data as a reference. 

## Introduction

This data set, in theory, would help a school find risk factors in students before a large assessment. By being able to tell which predictors are important, schools can focus on giving the extra support high-risk students need before an examination. 

There are four grade classes and we are trying to predict which one each student falls into. The highest grade class is zero, which is an  'A' (GPA >= 3.5). Grade class 1 is a 'B' (3.0 <= GPA < 3.5). Grade class 2 is a  'C' (2.5 <= GPA < 3.0). Grade class 3 is a 'D' (2.0 <= GPA < 2.5). Grade class 4 is a  'F' (GPA < 2.0), so the student is failing. It's important to mention what each grade class represents, as without explanation the category is vague.

## Exploratory Data Analysis

The original data set contains 2392 students, and 15 different columns of variables. Let's first look at the first few rows of the data set.
```{r}
# View the first few rows
data |>
  head() |>
  kable()
```
### Univariate Analysis

Because we are trying to predict grade class, it's important to see if the distribution of grade class is fairly proportional. This is important to see as we need to know if we should stratify this variable in our data splits.
```{r}
# Check distribution of gradeclass in the full data set
data |>
  count(GradeClass) |>
  mutate(proportion = n / sum(n)) |>
  kable() |>
  kable_styling()
```
It's clear that stratification is necessary as 50% of the students have a Grade Class of four, which means 50% of these students are failing. It is more rare for a student to get a good grade then a bad one.

Let's look at some of the other categorical variables, this time in our training data.
```{r}
p1 <- train |>
  mutate(Extracurricular = case_when(
    Extracurricular == 0 ~ "No",
    Extracurricular == 1 ~ "Yes"
  ),
  Extracurricular = factor(Extracurricular, levels = c("No", "Yes")),
    GradeClass = as.factor(GradeClass))
  ggplot(p1, aes(x=Extracurricular)) + 
  geom_bar() + 
  labs(
    title = "Participates in Extracurriculars",
    x = "",
    y = "Count"
  ) + 
  coord_flip()

p2 <- train |>
  mutate(ParentalEducation = case_when(
    ParentalEducation == 0 ~ "None",
    ParentalEducation == 1 ~ "High School",
    ParentalEducation == 2 ~ "Some College",
    ParentalEducation == 3 ~ "Bachelor's",
    ParentalEducation == 4 ~ "Higher"
  ),
   ParentalEducation = factor(ParentalEducation, levels = c("None", "High School", "Some College", "Bachelor's", "Higher")),
    GradeClass = as.factor(GradeClass))

  ggplot(p2, aes(x = ParentalEducation)) +
  geom_bar() +
  labs(
    title = "Parental Education",
    y = "Count",
    x = ""
  )

p3 <- train |>
  mutate(ParentalSupport = case_when(
    ParentalSupport == 0 ~ "None",
    ParentalSupport == 1 ~ "Low",
    ParentalSupport== 2 ~ "Moderate",
    ParentalSupport== 3 ~ "High",
    ParentalSupport== 4 ~ "Very High"
  ),
   ParentalSupport = factor(ParentalSupport, levels = c("None", "Low", "Moderate", "High", "Very High")),
    GradeClass = as.factor(GradeClass))
 

  ggplot(p3, aes(x = ParentalSupport),
                 fill = "pink") + 
  geom_bar() + 
  labs(
        title = "Parental Support Level",
    x = "Parental Support",
    y = "Count"
  ) + 
  coord_flip()

p4 <- train |>
  mutate(Tutoring = case_when(
    Tutoring == 0 ~ "Not Tutored",
    Tutoring == 1 ~ "Tutored"
  ),
   Tutoring = factor(Tutoring, levels = c("Not Tutored", "Tutored")),
    GradeClass = as.factor(GradeClass))

  ggplot(p4, aes(x = Tutoring)) + 
  geom_bar() + 
  labs(
    title = "Tutoring",
    x = "",
    y = "Count"
  )

p1

p2

p3

p4
```

These charts map out the distribution of other variables in the data set. The variables I chose were extracurriculars, parental education, parental support, and tutoring. While doing extracurriculars has a fairly even distribution, the other three variables do not. Surprisingly, there is a large amount of moderate/high parental support although there are a very few amount of kids who have high grades.

Let's continue to view individual distributions

```{r}
# Gender
p4 <- train |>
  mutate(Gender = case_when(
    Gender == 0 ~ "Male",
    Gender == 1 ~ "Female"
  ))
  ggplot(p4, aes(x= Gender)) + 
  geom_bar() + 
  labs(
    title = "Gender",
    x = "",
    y = "Count"
  ) 
  
# Ethnicity
  
  p5 <- train |>
  mutate(Ethnicity = case_when(
    Ethnicity == 0 ~ "Caucasian",
    Ethnicity == 1 ~ "African",
    Ethnicity == 2 ~ "Asian",
    Ethnicity == 3 ~ "Other"
  ),
    Ethnicity = factor(Ethnicity, levels = c("Caucasian", "African", "Asian", "Other")),
    GradeClass = as.factor(GradeClass))
  ggplot(p5, aes(x = Ethnicity)) + 
  geom_bar() + 
  labs(
    title = "Ethnicity",
    x = "",
    y = "Count"
  ) 
  
# Age Distribution

ggplot(train, aes(x = Age)) + 
  geom_bar() + 
  labs(
    title = "Age",
    x = "",
    y = "Count"
  ) 
 

```


### Multivariate Analysis

We have a grasp on individual distributions, now let's view the relationships between grade class and these variables.

Let's begin with categorical x categorical analysis. The variables we will be distributing alongside GradeClass are extracurriculars, parental education, parental support, and ethnicity. 

```{r}

ggplot(p5, aes(x = Ethnicity, fill = GradeClass)) + 
  geom_bar(position = "stack") + 
  labs(
    title = "Ethnicity and Grade Class",
    x = "Ethnicity",
    y = ""
  )  +
  theme_minimal()

ggplot(p2, aes(x = ParentalEducation, fill = GradeClass)) + 
  geom_bar(position = "stack") + 
  labs(
    title = "Parental Education and Grade Class",
    x = "Parental Education",
    y = ""
  )  +
  theme_minimal()

ggplot(p3, aes(x = ParentalSupport, fill = GradeClass)) + 
  geom_bar(position = "stack") + 
  labs(
    title = "Parental Support and Grade Class",
    x = "Parental Support",
    y = ""
  )  +
  theme_minimal()

ggplot(p1, aes(x = Extracurricular, fill = GradeClass)) + 
  geom_bar(position = "stack") + 
  labs(
    title = "Extracurriculars and Grade Class",
    x = "Extracurriculars",
    y = ""
  )  +
  theme_minimal()

```
It appears that extracurriculars appears to be relatively proportional, so it may not be the best parameter to focus on. Parental Support seems to make a difference, especially in the "high" column as compared to the "moderate", "low", and "none" columns.

Next let's look at a categorical and numerical boxplot, wherein grade class is compared to absences. 
```{r}
ggplot(train, aes(x = GradeClass, y = Absences)) + 
  geom_boxplot(fill = "pink", color = "black") + 
  labs(
    title = "Absences by Grade Class",
    x = "Grade Class",
    y = "Absences"
  ) + 
  theme_minimal()

ggplot(train, aes(x = GradeClass, y = StudyTimeWeekly)) + 
  geom_boxplot(fill = "pink", color = "black") + 
  labs(
    title = "Study Time Weekly by Grade Class",
    x = "Grade Class",
    y = "Study Time Weekly"
  ) + 
  theme_minimal()

```
There are clear associations between the numerical data like study time and absences and grade class. More absences equates to a lower grade, while more study time equates to a higher one. This is important information that we can use during tuning to get more accurate predictions.

## Model Construction

One important part about pre-processing with this data set specifically is that GPA needs to be removed in order to predict class grade. GPA is a variable that represents their future final GPA, so using it to predict grade class would essentially be cheating as you already have the answer to where their grade class is going to be. 


### Random Forest

Creating a random forest model will allow us to use machine learning to predict the grade class of a student.
Let's build, then tune.
```{r}
# Specification
rf_tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
  ) |>
  set_engine("ranger") |>
  set_mode("classification")

# Recipe
rf_tune_rec <- recipe(GradeClass ~ ., data = train) |>
  step_rm(GPA) |>
   step_mutate(GradeClass = as.factor(GradeClass))
  
# Workflow
  rf_wf <- workflow() |>
  add_model(rf_tune_spec) |>
  add_recipe(rf_tune_rec)
  
# Cross Validation Folds
  set.seed(123)
  train_folds <- vfold_cv(train, v = 10)
  
```
```{r}
set.seed(345)
tune_res <- tune_grid(
  rf_wf,
  resamples = train_folds,
  metrics = metric_set(f_meas, mn_log_loss),
  grid = 20
)

tune_res
```
Let's see how this grid turned out using mn log loss as our metric to plot min n and mtry. This way we can see what number range do better and make our tuning more specific.

```{r}
tune_res |>
  autoplot()
```

```{r}
tune_res |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss")|>
  select(mean, min_n, mtry) |>
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) |>
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "log loss")
```
From our charts, we can see that min_n in the 20-30 range produce better results, while mtry in the 5-10 range also produce better log loss results. A lower log loss is better. Let's make a regular systematic grid across this range of hyperparameter values.
```{r}
rf_grid <- grid_regular(
  mtry(range = c(5, 10)),
  min_n(range = c(20, 30)),
  levels = 5
)
```
Let's tune one more time with this new grid.
```{r}
set.seed(456)
regular_res <- tune_grid(
  rf_wf,
  resamples = train_folds,
  grid = rf_grid,
  metrics = metric_set(f_meas, mn_log_loss)
)

regular_res

collect_metrics(regular_res)
```
Let's gather our best fit for the metric "log_loss". 

```{r}
regular_res |>
  show_best(metric = "mn_log_loss")

best_params <- regular_res |>
  select_best(metric = "mn_log_loss")

rf_final_wf <- rf_wf |>
  finalize_workflow(best_params)

rf_best_fit <- rf_final_wf |>
  fit(train)

# Finalize workflow with best params
rf_best_fit <- last_fit(rf_final_wf, split = data_splits)
```

It appears our best is .62. Random is 1.609 as we have five classes, so .62 is significantly lower and a good mean. 

We can create a confusion matrix to see what classes our model struggles on predicting, and which ones it succeeds in.
```{r}
rf_best_fit|>
  collect_predictions() |>
  conf_mat(truth = GradeClass, estimate = .pred_class)

```
The results are unsurprising. The model does best in predicting the grade classes with more students in them. For example, it predicting 220 class 4's correct, and only 24 wrong, which is a much better ratio then in class 0 where it predicted five right and 16 wrong.

Let's look at our f_meas confusion matrix and mean to see if it's any better at predicting the lower grade classes.

```{r}
regular_res |>
  show_best(metric = "f_meas")

best_params_f <- regular_res |>
  select_best(metric = "f_meas")

rf_final_wf_f <- rf_wf |>
  finalize_workflow(best_params_f)

rf_best_fit_f <- rf_final_wf_f |>
  fit(train)

# Finalize workflow with best params
rf_best_fit_f <- last_fit(rf_final_wf_f, split = data_splits)

rf_best_fit_f |>
  collect_predictions() |>
  conf_mat(truth = GradeClass, estimate = .pred_class)
```
At least we checked, but it appears that this is more poor at predicting grade class zero then log_loss. Let's try a different machine learning model to see if we have better luck.

### XGBoost

XGBoost (Extreme Gradient Boosting) is a classification model. The reason we're going with this model next is because we can use "scale_pos_weight" to help with our rare classes. There will be a lot of tuning of model hyperparameters, so let's see how this goes.

```{r}
# Specification
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(),                    
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune(),
) |>
  set_engine("xgboost", scale_pos_weight = tune()) |>
  set_mode("classification")

# Recipe
xgb_rec <- recipe(GradeClass ~ ., data = train) |>
  step_rm(GPA) |>
  step_mutate(GradeClass = as.factor(GradeClass))

# Workflow
xgb_wf <- workflow() |>
  add_recipe(xgb_rec) |>
  add_model(xgb_spec)

xgb_wf

```
Now let's create our grid that we'll use for tuning:

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), train),
  learn_rate(),
  scale_pos_weight(),
  size = 30
)

xgb_grid
```
Now we can tune using this grid and parallel processing (this will likely take a while).
```{r}
doParallel::registerDoParallel()

set.seed(123)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = train_folds,
  grid = xgb_grid,
  metrics = metric_set(mn_log_loss, f_meas),
  control = control_grid(save_pred = TRUE)
)

xgb_res
```
Let's collect our metrics
```{r}
collect_metrics(xgb_res)
```
And then display our best for log loss

```{r}
show_best(xgb_res, metric = "mn_log_loss")

best_mn <- select_best(xgb_res, metric = "mn_log_loss")
best_mn
```
Now let's apply these best hyperparameters and finalize a new workflow. With this, our workflow is now set up with new optimized parameters.

```{r}
final_xgb <- finalize_workflow(
  xgb_wf,
  best_mn
)

final_xgb
```
Now we can do a final fit and collect the metrics to see if this model is better than our random forest.

```{r}
final_xgb_res <- last_fit(final_xgb, data_splits, metrics = metric_set(mn_log_loss, accuracy, f_meas))


collect_metrics(final_xgb_res)
```
Well. It didn't get better! This is most likely due to the fact that we specifically tuned on our random forest, using ranges, but not here. The real test is whether or not the model is good at predicting class zero and one.

```{r}
final_xgb_res |>
  collect_predictions() |>
  conf_mat(truth = GradeClass, estimate = .pred_class)
```
From the confusion matrix, it appears that the xgboost model did not improve finding the rarer classes. This is unfortunate but can happen.

## Conclusion

It appears that our random forest model is the better model (likely due to specific tuning methods), despite using scale_pos_weight in the xgboost model. The log loss estimate for the random forest model was about .1 less than the one in the xgboost model, rendering its performance as better.

In the future, I'd like to try and do more specific tuning with the xgboost model to see if I can make it better at predicting grade classes zero and one. If I can't, I would try and construct another model that can, and use a stacking method in order to get more accurate predictions for ALL grade classes. 

## References

üìö Students Performance Dataset üìö. (2024). Rabie El Kharoua. Kaggle.

Silge, Julia. ‚ÄúTuning Random Forest Hyperparameters with #Tidytuesday Trees Data.‚Äù Julia Silge, 26 Mar. 2020, juliasilge.com/blog/sf-trees-random-tuning/. 

Silge, Julia. ‚ÄúTune XGBoost with Tidymodels and #tidytuesday Beach Volleyball.‚Äù Julia Silge, 21 May 2020, juliasilge.com/blog/xgboost-tune-volleyball/. 
